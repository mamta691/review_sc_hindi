{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamta691/review_sc_hindi/blob/main/face_recognition_siamese_w_triplet_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face Recognition**\n",
        "\n",
        "**Facial Recognition System** is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image.\n",
        "\n",
        "We'll be building a face recognition model that uses **Siamese Networks** to give us a distance value that indicates whether 2 images are same or different.\n",
        "\n",
        "#### **The Dataset**\n",
        "We'll be using the **Extracted Faces** from **face-recognition-dataset**, which is derived from the **LFW Dataset**.\n",
        "The Extracted Faces contains faces extracted from the base images using **Haar-Cascade Face-Detection** (CV2).\n",
        "- The dataset contains 1324 different individuals, with 2-50 images per person.\n",
        "- The images are of size (128,128,3) and are encoded in RGB.\n",
        "- Each folder and image is named with a number, i.e 0.jpg, 1.jpg"
      ],
      "metadata": {
        "id": "SpVOOtkHayK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading the Dataset**\n",
        "\n",
        "We're reading the folders and splitting them into **train and test set** for training purposes."
      ],
      "metadata": {
        "id": "fYsil6PIayK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "tf.__version__, np.__version__"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T11:31:49.940203Z",
          "iopub.execute_input": "2021-07-15T11:31:49.940752Z",
          "iopub.status.idle": "2021-07-15T11:31:56.203954Z",
          "shell.execute_reply.started": "2021-07-15T11:31:49.940674Z",
          "shell.execute_reply": "2021-07-15T11:31:56.202858Z"
        },
        "trusted": true,
        "id": "loR1UZ3-ayK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting random seeds to enable consistency while testing.\n",
        "random.seed(5)\n",
        "np.random.seed(5)\n",
        "tf.random.set_seed(5)\n",
        "\n",
        "ROOT = \"../input/face-recognition-dataset/Extracted Faces/Extracted Faces\"\n",
        "\n",
        "def read_image(index):\n",
        "    path = os.path.join(ROOT, index[0], index[1])\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:13.466486Z",
          "iopub.execute_input": "2021-07-14T11:47:13.46683Z",
          "iopub.status.idle": "2021-07-14T11:47:13.473812Z",
          "shell.execute_reply.started": "2021-07-14T11:47:13.466797Z",
          "shell.execute_reply": "2021-07-14T11:47:13.472903Z"
        },
        "trusted": true,
        "id": "D3E9dfGRayK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(directory, split=0.9):\n",
        "    folders = os.listdir(directory)\n",
        "    num_train = int(len(folders)*split)\n",
        "\n",
        "    random.shuffle(folders)\n",
        "\n",
        "    train_list, test_list = {}, {}\n",
        "\n",
        "    # Creating Train-list\n",
        "    for folder in folders[:num_train]:\n",
        "        num_files = len(os.listdir(os.path.join(directory, folder)))\n",
        "        train_list[folder] = num_files\n",
        "\n",
        "    # Creating Test-list\n",
        "    for folder in folders[num_train:]:\n",
        "        num_files = len(os.listdir(os.path.join(directory, folder)))\n",
        "        test_list[folder] = num_files\n",
        "\n",
        "    return train_list, test_list\n",
        "\n",
        "train_list, test_list = split_dataset(ROOT, split=0.9)\n",
        "print(\"Length of training list:\", len(train_list))\n",
        "print(\"Length of testing list :\", len(test_list))\n",
        "\n",
        "# train_list, test list contains the folder names along with the number of files in the folder.\n",
        "print(\"\\nTest List:\", test_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:13.475342Z",
          "iopub.execute_input": "2021-07-14T11:47:13.475785Z",
          "iopub.status.idle": "2021-07-14T11:47:19.625081Z",
          "shell.execute_reply.started": "2021-07-14T11:47:13.475754Z",
          "shell.execute_reply": "2021-07-14T11:47:19.623843Z"
        },
        "trusted": true,
        "id": "5c_Y76GCayK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Triplets\n",
        "\n",
        "We use the train and test list to create triplets of **(anchor, postive, negative)** face data, where positive is the same person and negative is a different person than anchor."
      ],
      "metadata": {
        "id": "zsv2aMg9ayK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_triplets(directory, folder_list, max_files=10):\n",
        "    triplets = []\n",
        "    folders = list(folder_list.keys())\n",
        "\n",
        "    for folder in folders:\n",
        "        path = os.path.join(directory, folder)\n",
        "        files = list(os.listdir(path))[:max_files]\n",
        "        num_files = len(files)\n",
        "\n",
        "        for i in range(num_files-1):\n",
        "            for j in range(i+1, num_files):\n",
        "                anchor = (folder, f\"{i}.jpg\")\n",
        "                positive = (folder, f\"{j}.jpg\")\n",
        "\n",
        "                neg_folder = folder\n",
        "                while neg_folder == folder:\n",
        "                    neg_folder = random.choice(folders)\n",
        "                neg_file = random.randint(0, folder_list[neg_folder]-1)\n",
        "                negative = (neg_folder, f\"{neg_file}.jpg\")\n",
        "\n",
        "                triplets.append((anchor, positive, negative))\n",
        "\n",
        "    random.shuffle(triplets)\n",
        "    return triplets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:19.627073Z",
          "iopub.execute_input": "2021-07-14T11:47:19.627501Z",
          "iopub.status.idle": "2021-07-14T11:47:19.636487Z",
          "shell.execute_reply.started": "2021-07-14T11:47:19.627455Z",
          "shell.execute_reply": "2021-07-14T11:47:19.63537Z"
        },
        "trusted": true,
        "id": "Oa5edohhayK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_triplet = create_triplets(ROOT, train_list)\n",
        "test_triplet  = create_triplets(ROOT, test_list)\n",
        "\n",
        "print(\"Number of training triplets:\", len(train_triplet))\n",
        "print(\"Number of testing triplets :\", len(test_triplet))\n",
        "\n",
        "print(\"\\nExamples of triplets:\")\n",
        "for i in range(5):\n",
        "    print(train_triplet[i])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:19.637616Z",
          "iopub.execute_input": "2021-07-14T11:47:19.638018Z",
          "iopub.status.idle": "2021-07-14T11:47:20.266922Z",
          "shell.execute_reply.started": "2021-07-14T11:47:19.63799Z",
          "shell.execute_reply": "2021-07-14T11:47:20.26587Z"
        },
        "trusted": true,
        "id": "MwlhhyrdayK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Batch-Generator\n",
        "\n",
        "Creating a **Batch-Generator** that converts the triplets passed into batches of face-data and **preproccesses** it before returning the data into seperate lists.\n",
        "\n",
        "**Parameters:**\n",
        "- Batch_size: Batch_size of the data to return\n",
        "- Preprocess: Whether to preprocess the data or not"
      ],
      "metadata": {
        "id": "7JEWjqNEayLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(triplet_list, batch_size=256, preprocess=True):\n",
        "    batch_steps = len(triplet_list)//batch_size\n",
        "\n",
        "    for i in range(batch_steps+1):\n",
        "        anchor   = []\n",
        "        positive = []\n",
        "        negative = []\n",
        "\n",
        "        j = i*batch_size\n",
        "        while j<(i+1)*batch_size and j<len(triplet_list):\n",
        "            a, p, n = triplet_list[j]\n",
        "            anchor.append(read_image(a))\n",
        "            positive.append(read_image(p))\n",
        "            negative.append(read_image(n))\n",
        "            j+=1\n",
        "\n",
        "        anchor = np.array(anchor)\n",
        "        positive = np.array(positive)\n",
        "        negative = np.array(negative)\n",
        "\n",
        "        if preprocess:\n",
        "            anchor = preprocess_input(anchor)\n",
        "            positive = preprocess_input(positive)\n",
        "            negative = preprocess_input(negative)\n",
        "\n",
        "        yield ([anchor, positive, negative])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:20.26836Z",
          "iopub.execute_input": "2021-07-14T11:47:20.268747Z",
          "iopub.status.idle": "2021-07-14T11:47:20.278041Z",
          "shell.execute_reply.started": "2021-07-14T11:47:20.268707Z",
          "shell.execute_reply": "2021-07-14T11:47:20.276897Z"
        },
        "trusted": true,
        "id": "sY6ecjJQayLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Data\n",
        "\n",
        "Plotting the data generated from **get_batch()** to see the results"
      ],
      "metadata": {
        "id": "CWF4DTRfayLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = 6\n",
        "\n",
        "f, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n",
        "\n",
        "for x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):\n",
        "    a,p,n = x\n",
        "    for i in range(num_plots):\n",
        "        axes[i, 0].imshow(a[i])\n",
        "        axes[i, 1].imshow(p[i])\n",
        "        axes[i, 2].imshow(n[i])\n",
        "        i+=1\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:20.279301Z",
          "iopub.execute_input": "2021-07-14T11:47:20.2796Z",
          "iopub.status.idle": "2021-07-14T11:47:22.914178Z",
          "shell.execute_reply.started": "2021-07-14T11:47:20.279572Z",
          "shell.execute_reply": "2021-07-14T11:47:22.913389Z"
        },
        "trusted": true,
        "id": "VYN-lKnrayLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the Model**\n",
        "\n",
        "Unlike a conventional CNN, the **Siamese Network** does not classify the images into certain categories or labels, rather it only finds out the distance between any two given images. If the images have the same label, then the network should learn the parameters, i.e. the weights and the biases in such a way that it should produce a smaller distance between the two images, and if they belong to different labels, then the distance should be larger\n",
        "\n",
        "![Siamese Network Image](https://miro.medium.com/max/2000/1*05hUCDHhnl4hdjqvdVTHtw.png)"
      ],
      "metadata": {
        "id": "wn_keIZoayLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend, layers, metrics\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:22.915785Z",
          "iopub.execute_input": "2021-07-14T11:47:22.916271Z",
          "iopub.status.idle": "2021-07-14T11:47:23.116986Z",
          "shell.execute_reply.started": "2021-07-14T11:47:22.916239Z",
          "shell.execute_reply": "2021-07-14T11:47:23.115932Z"
        },
        "trusted": true,
        "id": "LkwBk6GMayLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "The **Encoder** is responsible for converting the passed images into their feature vectors. We're using a pretrained model, **Xception model** which is based on **Inception_V3 model.** By using transfer learning, we can significantly reduce the training time and size of the dataset.\n",
        "\n",
        "The Model is connected to **Fully Connected (Dense)** layers and the last layer normalises the data using **L2 Normalisation**. *(L2 Normalisation is a technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1)*"
      ],
      "metadata": {
        "id": "7vRc7-OWayLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(input_shape):\n",
        "    \"\"\" Returns the image encoding model \"\"\"\n",
        "\n",
        "    pretrained_model = Xception(\n",
        "        input_shape=input_shape,\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        pooling='avg',\n",
        "    )\n",
        "\n",
        "    for i in range(len(pretrained_model.layers)-27):\n",
        "        pretrained_model.layers[i].trainable = False\n",
        "\n",
        "    encode_model = Sequential([\n",
        "        pretrained_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
        "    ], name=\"Encode_Model\")\n",
        "    return encode_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:47:23.118374Z",
          "iopub.execute_input": "2021-07-14T11:47:23.118677Z",
          "iopub.status.idle": "2021-07-14T11:47:23.12684Z",
          "shell.execute_reply.started": "2021-07-14T11:47:23.11865Z",
          "shell.execute_reply": "2021-07-14T11:47:23.125665Z"
        },
        "trusted": true,
        "id": "l3B41onbayLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Siamese Network\n",
        "\n",
        "We're creating a **Siamese Network** that takes 3 input images, (anchor, postive, negative) and uses the encoder above to encode the images to their feature vectors. Those features are passed to a distance layer which computes the distance between **(anchor, positive)** and **(anchor, negative)** pairs.\n",
        "\n",
        "We'll be defining a custom layer to compute the distance.\n",
        "\n",
        "**Distance Formula**:\n",
        "\n",
        "![image.png](attachment:5e83389c-697d-4b02-8b39-81f5eba0ba9e.png)"
      ],
      "metadata": {
        "id": "Di86gvI0ayLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceLayer(layers.Layer):\n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "\n",
        "def get_siamese_network(input_shape = (128, 128, 3)):\n",
        "    encoder = get_encoder(input_shape)\n",
        "\n",
        "    # Input Layers for the images\n",
        "    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n",
        "    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n",
        "    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n",
        "\n",
        "    ## Generate the encodings (feature vectors) for the images\n",
        "    encoded_a = encoder(anchor_input)\n",
        "    encoded_p = encoder(positive_input)\n",
        "    encoded_n = encoder(negative_input)\n",
        "\n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    distances = DistanceLayer()(\n",
        "        encoder(anchor_input),\n",
        "        encoder(positive_input),\n",
        "        encoder(negative_input)\n",
        "    )\n",
        "\n",
        "    # Creating the Model\n",
        "    siamese_network = Model(\n",
        "        inputs  = [anchor_input, positive_input, negative_input],\n",
        "        outputs = distances,\n",
        "        name = \"Siamese_Network\"\n",
        "    )\n",
        "    return siamese_network\n",
        "\n",
        "siamese_network = get_siamese_network()\n",
        "siamese_network.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T04:43:53.53274Z",
          "iopub.execute_input": "2021-07-14T04:43:53.533274Z",
          "iopub.status.idle": "2021-07-14T04:43:57.276347Z",
          "shell.execute_reply.started": "2021-07-14T04:43:53.533231Z",
          "shell.execute_reply": "2021-07-14T04:43:57.275021Z"
        },
        "trusted": true,
        "id": "ZiF9CI9gayLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(siamese_network, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T04:43:57.277504Z",
          "iopub.execute_input": "2021-07-14T04:43:57.277769Z",
          "iopub.status.idle": "2021-07-14T04:43:57.42519Z",
          "shell.execute_reply.started": "2021-07-14T04:43:57.277747Z",
          "shell.execute_reply": "2021-07-14T04:43:57.424151Z"
        },
        "trusted": true,
        "id": "sJjVPoe8ayLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting everything together\n",
        "\n",
        "We now need to implement a model with custom training loop and loss function so we can compute the **triplet loss** using the three embeddings produced by the Siamese network.\n",
        "\n",
        "We'll create a **Mean metric** instance to track the loss of the training process.\n",
        "\n",
        "**Triplet Loss Function:**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1838/0*AX2TSZNk19_gDgTN.png\" alt=\"Loss Formula\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "vCdw1cHtayLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseModel(Model):\n",
        "    # Builds a Siamese model based on a base-model\n",
        "    def __init__(self, siamese_network, margin=1.0):\n",
        "        super(SiameseModel, self).__init__()\n",
        "\n",
        "        self.margin = margin\n",
        "        self.siamese_network = siamese_network\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        # Get the two distances from the network, then compute the triplet loss\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics so the reset_states() can be called automatically.\n",
        "        return [self.loss_tracker]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T04:43:57.427196Z",
          "iopub.execute_input": "2021-07-14T04:43:57.427578Z",
          "iopub.status.idle": "2021-07-14T04:43:57.436532Z",
          "shell.execute_reply.started": "2021-07-14T04:43:57.427546Z",
          "shell.execute_reply": "2021-07-14T04:43:57.435956Z"
        },
        "trusted": true,
        "id": "wXnB96fKayLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n",
        "siamese_model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T04:43:57.447966Z",
          "iopub.execute_input": "2021-07-14T04:43:57.448415Z",
          "iopub.status.idle": "2021-07-14T04:43:57.47386Z",
          "shell.execute_reply.started": "2021-07-14T04:43:57.448376Z",
          "shell.execute_reply": "2021-07-14T04:43:57.473142Z"
        },
        "trusted": true,
        "id": "W0z2FsElayLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**\n",
        "\n",
        "We'll now be training the siamese_model on batches of triplets. We'll print the training loss, along with additional metrics from testing every epoch. The model weights will also be saved whenever it outperforms the previous max_accuracy.\n",
        "\n",
        "We're hoping to collect more metrics about the model to evaluate how to increase the accuracy of the model. The epochs have been set to avoid going over Kaggle's time constraint.\n",
        "\n",
        "### Test Function\n",
        "\n",
        "**test_on_triplets()** function will be responsible for testing the model on test_triplets. It'll collect metrics **(accuracy, means, stds)** by predicting on the train data. We'll also be printing the Accuracy of the model after testing."
      ],
      "metadata": {
        "id": "cuYopTMzayLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_on_triplets(batch_size = 256):\n",
        "    pos_scores, neg_scores = [], []\n",
        "\n",
        "    for data in get_batch(test_triplet, batch_size=batch_size):\n",
        "        prediction = siamese_model.predict(data)\n",
        "        pos_scores += list(prediction[0])\n",
        "        neg_scores += list(prediction[1])\n",
        "\n",
        "    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n",
        "    ap_mean = np.mean(pos_scores)\n",
        "    an_mean = np.mean(neg_scores)\n",
        "    ap_stds = np.std(pos_scores)\n",
        "    an_stds = np.std(neg_scores)\n",
        "\n",
        "    print(f\"Accuracy on test = {accuracy:.5f}\")\n",
        "    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T18:13:39.458795Z",
          "iopub.execute_input": "2021-07-13T18:13:39.459181Z",
          "iopub.status.idle": "2021-07-13T18:13:39.465734Z",
          "shell.execute_reply.started": "2021-07-13T18:13:39.459091Z",
          "shell.execute_reply": "2021-07-13T18:13:39.464963Z"
        },
        "trusted": true,
        "id": "mlhqFqeEayLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_all = False\n",
        "epochs = 256\n",
        "batch_size = 128\n",
        "\n",
        "max_acc = 0\n",
        "train_loss = []\n",
        "test_metrics = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    t = time.time()\n",
        "\n",
        "    # Training the model on train data\n",
        "    epoch_loss = []\n",
        "    for data in get_batch(train_triplet, batch_size=batch_size):\n",
        "        loss = siamese_model.train_on_batch(data)\n",
        "        epoch_loss.append(loss)\n",
        "    epoch_loss = sum(epoch_loss)/len(epoch_loss)\n",
        "    train_loss.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n",
        "    print(f\"Loss on train    = {epoch_loss:.5f}\")\n",
        "\n",
        "    # Testing the model on test data\n",
        "    metric = test_on_triplets(batch_size=batch_size)\n",
        "    test_metrics.append(metric)\n",
        "    accuracy = metric[0]\n",
        "\n",
        "    # Saving the model weights\n",
        "    if save_all or accuracy>=max_acc:\n",
        "        siamese_model.save_weights(\"siamese_model\")\n",
        "        max_acc = accuracy\n",
        "\n",
        "# Saving the model after all epochs run\n",
        "siamese_model.save_weights(\"siamese_model-final\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T18:13:39.467268Z",
          "iopub.execute_input": "2021-07-13T18:13:39.468032Z",
          "iopub.status.idle": "2021-07-13T18:15:48.010723Z",
          "shell.execute_reply.started": "2021-07-13T18:13:39.467993Z",
          "shell.execute_reply": "2021-07-13T18:15:48.009861Z"
        },
        "trusted": true,
        "id": "KmUbx7XNayLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Model**\n"
      ],
      "metadata": {
        "id": "arOD2X-_ayLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(loss, metrics):\n",
        "    # Extracting individual metrics from metrics\n",
        "    accuracy = metrics[:, 0]\n",
        "    ap_mean  = metrics[:, 1]\n",
        "    an_mean  = metrics[:, 2]\n",
        "    ap_stds  = metrics[:, 3]\n",
        "    an_stds  = metrics[:, 4]\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "\n",
        "    # Plotting the loss over epochs\n",
        "    plt.subplot(121)\n",
        "    plt.plot(loss, 'b', label='Loss')\n",
        "    plt.title('Training loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the accuracy over epochs\n",
        "    plt.subplot(122)\n",
        "    plt.plot(accuracy, 'r', label='Accuracy')\n",
        "    plt.title('Testing Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "\n",
        "    # Comparing the Means over epochs\n",
        "    plt.subplot(121)\n",
        "    plt.plot(ap_mean, 'b', label='AP Mean')\n",
        "    plt.plot(an_mean, 'g', label='AN Mean')\n",
        "    plt.title('Means Comparision')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the accuracy\n",
        "    ap_75quartile = (ap_mean+ap_stds)\n",
        "    an_75quartile = (an_mean-an_stds)\n",
        "    plt.subplot(122)\n",
        "    plt.plot(ap_75quartile, 'b', label='AP (Mean+SD)')\n",
        "    plt.plot(an_75quartile, 'g', label='AN (Mean-SD)')\n",
        "    plt.title('75th Quartile Comparision')\n",
        "    plt.legend()\n",
        "\n",
        "test_metrics = np.array(test_metrics)\n",
        "plot_metrics(train_loss, test_metrics)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T18:16:43.418481Z",
          "iopub.execute_input": "2021-07-13T18:16:43.418813Z",
          "iopub.status.idle": "2021-07-13T18:16:44.143529Z",
          "shell.execute_reply.started": "2021-07-13T18:16:43.418771Z",
          "shell.execute_reply": "2021-07-13T18:16:44.142712Z"
        },
        "trusted": true,
        "id": "l19h0HghayLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using the Model**\n",
        "\n",
        "Now that we've finished training our model, we need to **extract the encoder** so that we can use it to encode images and then get use the feature vectors to compute the distance between those images.\n",
        "\n",
        "We'll also be saving the encoder for latter use."
      ],
      "metadata": {
        "id": "PFmU4BbwayLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_encoder(model):\n",
        "    encoder = get_encoder((128, 128, 3))\n",
        "    i=0\n",
        "    for e_layer in model.layers[0].layers[3].layers:\n",
        "        layer_weight = e_layer.get_weights()\n",
        "        encoder.layers[i].set_weights(layer_weight)\n",
        "        i+=1\n",
        "    return encoder\n",
        "\n",
        "encoder = extract_encoder(siamese_model)\n",
        "encoder.save_weights(\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T04:44:39.997113Z",
          "iopub.execute_input": "2021-07-14T04:44:39.997416Z",
          "iopub.status.idle": "2021-07-14T04:44:45.295765Z",
          "shell.execute_reply.started": "2021-07-14T04:44:39.99739Z",
          "shell.execute_reply": "2021-07-14T04:44:45.294958Z"
        },
        "trusted": true,
        "id": "bk2eAa9VayLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify Images\n",
        "\n",
        "To compute the distance between the encodings of the images, we'll be using distance formula. Distance over a certain threshold to be \"different\" and below the threshold as \"same\"."
      ],
      "metadata": {
        "id": "cC8lxiCEayLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_images(face_list1, face_list2, threshold=1.3):\n",
        "    # Getting the encodings for the passed faces\n",
        "    tensor1 = encoder.predict(face_list1)\n",
        "    tensor2 = encoder.predict(face_list2)\n",
        "\n",
        "    distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n",
        "    prediction = np.where(distance<=threshold, 0, 1)\n",
        "    return prediction"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:48:09.860064Z",
          "iopub.execute_input": "2021-07-14T11:48:09.860426Z",
          "iopub.status.idle": "2021-07-14T11:48:09.867504Z",
          "shell.execute_reply.started": "2021-07-14T11:48:09.86039Z",
          "shell.execute_reply": "2021-07-14T11:48:09.866359Z"
        },
        "trusted": true,
        "id": "a9TQo-zKayLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelMetrics(pos_list, neg_list):\n",
        "    true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n",
        "    pred = np.append(pos_list, neg_list)\n",
        "\n",
        "    # Compute and print the accuracy\n",
        "    print(f\"\\nAccuracy of model: {accuracy_score(true, pred)}\\n\")\n",
        "\n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(true, pred)\n",
        "\n",
        "    categories  = ['Similar','Different']\n",
        "    names = ['True Similar','False Similar', 'False Different','True Different']\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
        "\n",
        "\n",
        "pos_list = np.array([])\n",
        "neg_list = np.array([])\n",
        "\n",
        "for data in get_batch(test_triplet, batch_size=256):\n",
        "    a, p, n = data\n",
        "    pos_list = np.append(pos_list, classify_images(a, p))\n",
        "    neg_list = np.append(neg_list, classify_images(a, n))\n",
        "    break\n",
        "\n",
        "ModelMetrics(pos_list, neg_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T11:49:52.542742Z",
          "iopub.execute_input": "2021-07-14T11:49:52.543151Z",
          "iopub.status.idle": "2021-07-14T11:49:52.758502Z",
          "shell.execute_reply.started": "2021-07-14T11:49:52.543115Z",
          "shell.execute_reply": "2021-07-14T11:49:52.757375Z"
        },
        "trusted": true,
        "id": "jcDB6uQ8ayLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- FaceNet: A Unified Embedding for Face Recognition and Clustering: https://arxiv.org/abs/1503.03832\n",
        "- Image similarity estimation using a Siamese Network with a triplet loss: https://keras.io/examples/vision/siamese_network/\n",
        "- Celebrity Face Recognition: https://www.kaggle.com/ravehgillmore/celebrity-face-recognition/\n",
        "- Face Recognition using Siamese Networks: https://medium.com/wicds/face-recognition-using-siamese-networks-84d6f2e54ea4\n"
      ],
      "metadata": {
        "id": "SH7o4qxcayLY"
      }
    }
  ]
}